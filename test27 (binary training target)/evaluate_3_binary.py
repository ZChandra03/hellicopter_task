#!/usr/bin/env python3
"""
evaluate_3_max.py — binary‑pipeline edition (17 Jul 2025)
=========================================================
Evaluates the *Bayesian normative observer* plus the two trained
network families (**truth** & **norm**) on up to MAX_TEST_CSVS
test variants generated by *TaskConfig_Generator_binary.py*.

Run from the folder that contains:
    ├─ evaluate_3_max.py
    ├─ models/
    │   ├─ truth/seed_0/checkpoint_best.pt  (… etc.)
    │   └─ norm /seed_0/checkpoint_best.pt
    └─ variants/
        └─ test/test_00.csv …

Usage
-----
    python evaluate_3_max.py               # uses all test CSVs
    python evaluate_3_max.py 15            # cap at 15 CSVs
"""
from __future__ import annotations
import ast, os, glob, sys
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
import torch

from rnn_models import GRUModel
from NormativeModel import BayesianObserver

# ─────────────────────── configuration ────────────────────────────────
DEVICE        = torch.device("cuda" if torch.cuda.is_available() else "cpu")
BASE_DIR      = os.path.dirname(os.path.abspath(__file__))
MODELS_DIR    = os.path.join(BASE_DIR, "models")
VARIANT_ROOT  = os.path.join(BASE_DIR, "variants", "test")       # ← NEW
MODEL_TYPES   = ["truth", "norm"]                                # ← NEW
SEEDS         = [0, 1, 2]                                        # first 3
MAX_TEST_CSVS = int(sys.argv[1]) if len(sys.argv) > 1 else None  # optional

HS_GRID, MU1, MU2 = np.arange(0, 1.05, 0.05), -1, 1
EPS = 1e-10

# ─────────────────────── helpers ───────────────────────────────────────
def _clip(p: float) -> float:
    """Avoid log(0) in loss calc."""
    return float(np.clip(p, EPS, 1 - EPS))

def _list_test_csvs(max_csvs: int | None = None) -> List[str]:
    """Return up to *max_csvs* test CSV paths (sorted)."""
    paths = sorted(glob.glob(os.path.join(VARIANT_ROOT, "test_*.csv")))  # ← NEW
    if not paths:
        raise FileNotFoundError(f"No test CSVs found under {VARIANT_ROOT}")
    return paths[:max_csvs] if max_csvs is not None else paths

def _load_model(fam: str, seed: int) -> torch.nn.Module:
    ckpt = os.path.join(MODELS_DIR, fam, f"seed_{seed}", "checkpoint_best.pt")
    if not os.path.isfile(ckpt):
        raise FileNotFoundError(f"[missing ckpt] {ckpt}")
    hp = {"n_input": 1, "n_rnn": 128}
    model = GRUModel(hp).to(DEVICE)
    model.load_state_dict(torch.load(ckpt, map_location=DEVICE))
    model.eval()
    return model

# ─────────────────────── per‑CSV evaluation ───────────────────────────
def _evaluate_csv(csv_path: str, model: torch.nn.Module | None) -> List[Dict]:
    """Return list of per‑trial dicts. If *model* is None → normative only."""
    df = pd.read_csv(csv_path)
    recs: List[Dict] = []

    with torch.no_grad():
        for _, row in df.iterrows():
            # unpack trial -----------------------------------------------------
            evid = row["evidence"]
            if not isinstance(evid, list):
                evid = ast.literal_eval(str(evid))
            sigma     = float(row.get("sigma", row.get("noise", 0.0)))
            haz_true  = float(row.get("trueHazard", row.get("hazard", row.get("hazardRate"))))
            rep_true  = int(row.get("trueReport", row.get("report", 0)))
            y_rep     = 0.5 * (rep_true + 1)

            # normative observer -----------------------------------------------
            L_haz, L_state, rep_norm, _ = BayesianObserver(
                evid, MU1, MU2, sigma, HS_GRID.copy())
            p_state2_norm = _clip(L_state[1, -1])
            haz_est_norm  = float(np.dot(HS_GRID, L_haz[:, -1]))
            p_haz_norm    = _clip(haz_est_norm)

            rec = {
                "rep_corr_norm": int(rep_norm == rep_true),
                "haz_acc_norm" : int(abs(haz_est_norm - haz_true) <= 0.10),
                "haz_hilo_norm": int((haz_est_norm > .5) == (haz_true > .5)),
                "loss_norm"    : (
                    -(y_rep * np.log(p_state2_norm) + (1 - y_rep) * np.log(1 - p_state2_norm))
                    + 0.5 * (-(haz_true * np.log(p_haz_norm) + (1 - haz_true) * np.log(1 - p_haz_norm)))
                ),
                "haz_err_norm" : haz_est_norm - haz_true,
            }

            # network prediction -----------------------------------------------
            if model is not None:
                x = torch.tensor(evid, dtype=torch.float32).unsqueeze(0).unsqueeze(-1).to(DEVICE)
                loc_logits, haz_logits = model(x)
                loc_last   = loc_logits[0, -1, 0]
                p_state2_net = _clip(torch.sigmoid(loc_last).item())
                p_haz_net    = _clip(torch.sigmoid(haz_logits)[0, 0].item())
                rep_pred     = 1 if loc_last.item() > 0 else -1

                rec.update({
                    "rep_corr_net": int(rep_pred == rep_true),
                    "haz_acc_net" : int(abs(p_haz_net - haz_true) <= 0.10),
                    "haz_hilo_net": int((p_haz_net > .5) == (haz_true > .5)),
                    "loss_net"    : (
                        -(y_rep * np.log(p_state2_net) + (1 - y_rep) * np.log(1 - p_state2_net))
                        + 0.5 * (-(haz_true * np.log(p_haz_net) + (1 - haz_true) * np.log(1 - p_haz_net)))
                    ),
                    "haz_err_net" : p_haz_net - haz_true,
                })

            recs.append(rec)
    return recs

def _aggregate(recs: List[Dict], use_net: bool) -> Tuple[float, float, float, float]:
    """Return (rep_acc, haz_acc_±0.10, haz_hi/lo, loss)."""
    df = pd.DataFrame(recs)
    cols = ("rep_corr_net", "haz_acc_net", "haz_hilo_net", "loss_net") if use_net else (
           "rep_corr_norm", "haz_acc_norm", "haz_hilo_norm", "loss_norm")
    return tuple(df[c].mean() for c in cols)  # type: ignore[return-value]

# ─────────────────────── top‑level evals ───────────────────────────────
def evaluate_norm() -> Tuple[float, float, float, float, np.ndarray]:
    csvs = _list_test_csvs(MAX_TEST_CSVS)
    recs: List[Dict] = []
    for p in csvs:
        recs.extend(_evaluate_csv(p, model=None))
    rep, haz, hilo, loss = _aggregate(recs, use_net=False)
    errs = np.array([r["haz_err_norm"] for r in recs], dtype=float)
    return rep, haz, hilo, loss, errs

def evaluate_network(fam: str, seed: int, err_norm: np.ndarray) -> None:
    model = _load_model(fam, seed)
    csvs = _list_test_csvs(MAX_TEST_CSVS)
    recs: List[Dict] = []
    for p in csvs:
        recs.extend(_evaluate_csv(p, model))
    rep, haz, hilo, loss = _aggregate(recs, use_net=True)
    errs = np.array([r["haz_err_net"] for r in recs], dtype=float)

    r = float('nan') if errs.std() < 1e-12 or err_norm.std() < 1e-12 else float(
        np.corrcoef(errs, err_norm)[0, 1])

    tag_disp = f"{fam}_s{seed}".upper()
    print(f"{tag_disp:<12}: report {rep:6.3%} | hazard ±0.10 {haz:6.3%} | "
          f"hazard Hi/Lo {hilo:6.3%} | loss {loss:.4f} | r(NORM) = {r:+.4f}")

# ─────────────────────── entry point ───────────────────────────────────
def main():
    print("========= Bayesian Normative Baseline =========")
    rep_n, haz_n, hilo_n, loss_n, err_norm = evaluate_norm()
    print(f"NORM         : report {rep_n:6.3%} | hazard ±0.10 {haz_n:6.3%} | "
          f"hazard Hi/Lo {hilo_n:6.3%} | loss {loss_n:.4f}\n")

    print("========= Trained Networks (truth & norm) =========")
    for fam in MODEL_TYPES:
        for seed in SEEDS:
            try:
                evaluate_network(fam, seed, err_norm)
            except FileNotFoundError as e:
                print(f"[skip] {e}")

    print("\nEvaluation complete.")

if __name__ == "__main__":
    main()
