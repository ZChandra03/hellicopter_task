"""evaluate.py – accuracy of trained GRUModel split by hazard‑rate & noise
==========================================================================
Run from the project root *after* training:

    python evaluate.py

Outputs a tidy table with columns
    sigma | hazard | N | report_acc | predict_acc
where
    • report_acc  – accuracy of the *location* head (trueReport)
    • predict_acc – accuracy of the *hazard*   head (truePredict)

Assumes
-------
models/gru_checkpoint.pt    – saved by train.py
variants/testConfig_var*.csv – test variants generated by TaskConfig_Generator
"""
import os, ast
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
import torch

from gru_model import GRUModel

# -----------------------------------------------------------------------------
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
MODEL_PATH   = os.path.join(BASE_DIR, "models", "gru_trained", "checkpoint.pt")
VARIANT_DIR  = os.path.join(BASE_DIR, "variants")

# -----------------------------------------------------------------------------
def get_default_hp() -> Dict[str, int]:
    """Return the hyper‑params required to re‑instantiate the trained model."""
    return {
        "n_input": 1,
        "n_rnn"  : 128,
        # bidirectional = False by default in gru_model.py
    }

# -----------------------------------------------------------------------------
def load_model(hp: Dict[str, int], ckpt_path: str = MODEL_PATH) -> GRUModel:
    model = GRUModel(hp).to(DEVICE)
    state = torch.load(ckpt_path, map_location=DEVICE)
    model.load_state_dict(state)
    model.eval()
    return model

# -----------------------------------------------------------------------------
def _row_to_tensor(evidence_str: str) -> torch.Tensor:
    """Convert the stored Python‑repr string list → (1, T, 1) tensor."""
    x = torch.tensor(ast.literal_eval(evidence_str), dtype=torch.float32)
    return x.unsqueeze(0).unsqueeze(-1)   # (1, T, 1)

# -----------------------------------------------------------------------------
def evaluate_variant(model: GRUModel, csv_path: str) -> List[Dict]:
    """Run inference on *one* test CSV and collect per‑trial outcomes."""
    df = pd.read_csv(csv_path)
    records = []
    with torch.no_grad():
        for _, row in df.iterrows():
            x = _row_to_tensor(row["evidence"]).to(DEVICE)
            loc_logits, haz_logits = model(x)

            loc_pred =  1 if loc_logits[0, -1, 0].item() > 0 else -1
            haz_pred =  1 if haz_logits[0,  0   ].item() > 0 else -1

            records.append({
                "sigma"       : row["sigma"],
                "hazard"      : row["trueHazard"],
                "rep_correct" : int(loc_pred == row["trueReport"]),
                "haz_correct" : int(haz_pred == row["truePredict"]),
            })
    return records

# -----------------------------------------------------------------------------
def aggregate(records: List[Dict]) -> pd.DataFrame:
    df = pd.DataFrame(records)
    if df.empty:
        raise RuntimeError("No evaluation data found – check paths to testConfig_var*.csv")

    grouped = (
        df.groupby(["sigma", "hazard"])   # separate by noise & hazard‑rate
          .agg(N          =("rep_correct", "size"),
               report_acc =("rep_correct", "mean"),
               predict_acc=("haz_correct", "mean"))
          .reset_index()
          .sort_values(["sigma", "hazard"])
    )
    return grouped

# -----------------------------------------------------------------------------
def main(max_variants: int = 40):
    model = load_model(get_default_hp())

    all_records: List[Dict] = []
    for k in range(max_variants):
        csv_path = os.path.join(VARIANT_DIR, f"testConfig_var{k}.csv")
        if not os.path.isfile(csv_path):
            break
        all_records.extend(evaluate_variant(model, csv_path))

    table = aggregate(all_records)
    print("\nPer‑condition accuracy (two heads)\n" + "="*40)
    print(table.to_string(index=False, float_format="{:.3f}".format))

    # overall summary ---------------------------------------------------------
    overall = table.agg({"N": "sum",
                         "report_acc": lambda x: np.average(x, weights=table["N"]),
                         "predict_acc": lambda x: np.average(x, weights=table["N"])})
    print("\nOverall accuracy:\n  report  = {:.3%}\n  predict = {:.3%}".format(overall["report_acc"], overall["predict_acc"]))

# -----------------------------------------------------------------------------
if __name__ == "__main__":
    main()
